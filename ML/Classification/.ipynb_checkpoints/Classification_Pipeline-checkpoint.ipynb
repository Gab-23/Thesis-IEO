{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the python kernel we are using, should be the local one, not the one in the virtual environment. <br>\n",
    "NOTE: A proper virtual environment will be setup later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up main variables that will be called later on, for readibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_bool = True\n",
    "path_to_datasets = \"/home/ieo7429/Desktop/THESIS_GAB/outfiles/target_variables/ML_tables_regression_classification_BRCA_v2.RData\"\n",
    "mode = \"regrout\"\n",
    "\n",
    "cols_to_drop = [\"ampl_score\",\"del_score\",\"mean_log2FC\",\"weighted_log2FC\", \"joint_probability\", \"bool_diff_acc\", \n",
    "                \"sign_mean_log2FC_2\", \"sign_mean_log2FC_3\", \n",
    "                \"weighted_coef_of_var_log2FC\", \"weighted_log2FC\", \"bin\", \"Type\"]\n",
    "\n",
    "if \"ampl_score\" in cols_to_drop and \"del_score\" in cols_to_drop:\n",
    "    condition = \"without_CNA\"\n",
    "else:\n",
    "    condition = \"with CNA\"\n",
    "\n",
    "n_jobs = 60\n",
    "nthread = 60\n",
    "n_iter_bsearch = 50\n",
    "n_iter_rsearch = 100\n",
    "\n",
    "split_random_state = 489574\n",
    "classifier_seed = 3737\n",
    "classifier_random_state = 39473209\n",
    "hypertune_random_state_bsearch = 3847\n",
    "hypertune_random_state_rsearch = 49574\n",
    "kfold_random_state = 4909\n",
    "\n",
    "outer_cv = 5\n",
    "inner_cv = 3\n",
    "plot_variation = False\n",
    "plot_gain = False\n",
    "plot_shap = True\n",
    "\n",
    "X_train_filename = \"Classification_Output/classification_X_train_red\"; X_test_filename = \"Classification_Output/classification_X_test_red\"\n",
    "y_train_filename = \"Classification_Output/classification_y_train_red\"; y_test_filename = \"Classification_Output/classification_y_test_red\"\n",
    "bin_train_filename = \"Classification_Output/classification_bin_train_red\"; bin_test_filename = \"Classification_Output/classification_bin_test_red\"\n",
    "model_filename = \"Classification_Output/classification_final_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if install_bool:\n",
    "    ! pip install pickleshare\n",
    "    ! pip install pandas\n",
    "    ! pip install scikit-learn\n",
    "    ! pip install seaborn\n",
    "    ! pip install scipy\n",
    "    ! pip install --upgrade pip setuptools wheel\n",
    "    ! pip install rpy2\n",
    "    ! pip install shap\n",
    "    ! pip install \"numpy<=2.1\"\n",
    "    %pip install -U ipywidgets\n",
    "\n",
    "clear_output(wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import rpy2.robjects as ro\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings\n",
    "import pickle\n",
    "import pickleshare\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_datasets_from_r\n",
    "Reads \"ML.Tables\" .Rdata file and converts it to a nested dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_from_r(file_path):\n",
    "\n",
    "    \"\"\" \n",
    "    The function uses rpy2 to load datasets from an RData file.\n",
    "    In particular it expects a list of data.frames\n",
    "    It returns a dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    pandas2ri.activate()\n",
    "\n",
    "    ro.r['load'](file_path) # load the .RData\n",
    "\n",
    "    env = ro.globalenv # set the environment\n",
    "\n",
    "    r_list = env[list(env.keys())[0]] # take the list object\n",
    "\n",
    "    list_names = ro.r['names'](r_list) # take the names\n",
    "    list_names = [str(name) for name in list_names]\n",
    "\n",
    "    df_dict = {} # initialize outer dictionary\n",
    "    with (ro.default_converter + ro.pandas2ri.converter).context(): # start the conversion\n",
    "    # the conversion transforms a list of lists of lists of data.frames \n",
    "    # to a list of dictionaries of dictionaries of dictionaries of DataFrame\n",
    "        i = 0\n",
    "        for outer_dict in r_list:\n",
    "            df_dict[list_names[i]] = outer_dict\n",
    "            i += 1\n",
    "            \n",
    "    return(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gini_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute the gini coefficient (2 * AUC - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient_score(y_true, y_pred_prob, **kwargs):\n",
    "    y_score = y_pred_prob\n",
    "    auc_score = sk.metrics.roc_auc_score(y_true, y_score)\n",
    "    gini_coef = 2 * auc_score - 1\n",
    "    return gini_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_coefficient = sk.metrics.make_scorer(gini_coefficient_score, response_method = \"predict_proba\", greater_is_better= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_selector\n",
    "his function performs Incremental Feature Selection (IFS) to produce a reduced model <br> with best gini coefficient (2 * AUC - 1) performance with cross validation.\n",
    "Mean absolute SHAP values are used to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector_shap(feature_names, feature_importances, model, \n",
    "                          X_train, X_test, y_train, inner_cv,\n",
    "                          plot = True, verbose = True):\n",
    "\n",
    "    \"\"\"\n",
    "    The function takes as input feature names, feature importances, \n",
    "    a XGBoost Classifier object, the training and test dataset\n",
    "    and returns the reduced training and test datasets.\n",
    "    - plot parameter regulates whether or not to plot the Gini Coefficient vs number of feature plot\n",
    "    - verbose parameter regulates whether or not to produce a text output\n",
    "    \"\"\"\n",
    "\n",
    "    thresholds = np.sort(feature_importances) # sort the feature importances\n",
    "    num_features_list = [] # initialize list\n",
    "    gini_coef_list = [] # initialize list\n",
    "    \n",
    "    for threshold in thresholds: # iterate over thresholds\n",
    "        \n",
    "        vars_to_keep = np.where(feature_importances >= threshold)[0]\n",
    "        X_train_selected = X_train.iloc[:,vars_to_keep]\n",
    "\n",
    "        gini_coef_scores = sk.model_selection.cross_val_score(estimator = model, \n",
    "                                                              X = X_train_selected, y = y_train, \n",
    "                                                              scoring = gini_coefficient, cv = inner_cv)\n",
    "        mean_gini_coef_score = np.mean(gini_coef_scores)\n",
    "        \n",
    "        num_features_list.append(X_train_selected.shape[1]) # append\n",
    "        gini_coef_list.append(mean_gini_coef_score) # append\n",
    "\n",
    "        if verbose:\n",
    "            print(f'> threshold={threshold}, features={X_train_selected.shape[1]}, gini coefficient={mean_gini_coef_score}')\n",
    "        \n",
    "        if len(vars_to_keep) == 1:\n",
    "            break\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(num_features_list, gini_coef_list, marker='o')\n",
    "        plt.xlabel('Number of Selected Features')\n",
    "        plt.ylabel('cross validated mean Ginny coefficient')\n",
    "        plt.title('Gini coefficient vs. Number of Selected Features')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    optimal_threshold_index = np.argmax(gini_coef_list)\n",
    "    optimal_num_features = num_features_list[optimal_threshold_index]\n",
    "    \n",
    "    if (optimal_threshold_index) == 0:\n",
    "        optimal_threshold = 0\n",
    "    else:   \n",
    "        optimal_threshold = thresholds[optimal_threshold_index - 1]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"Number of Selected Features: {optimal_num_features}\")\n",
    "        print(f\"Gini coef at Optimal Threshold: {gini_coef_list[optimal_threshold_index]:.4f}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    n = 5\n",
    "    selected_features = feature_names[np.where(feature_importances > optimal_threshold)]\n",
    "    selected_features = [str(name) for name in selected_features]\n",
    "    discarded_features = [str(name) for name in feature_names if str(name) not in selected_features]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Selected features are: \\n\")\n",
    "        for i in range(0,len(selected_features), n):\n",
    "            print(\"  \".join(selected_features[i:i+n]))\n",
    "        print(\"\\n\")\n",
    "        print(\"Discarded features are: \\n\")\n",
    "        for i in range(0,len(discarded_features), n):\n",
    "            print(\"  \".join(discarded_features[i:i+n]))\n",
    "\n",
    "    X_train_reduced = X_train[selected_features]\n",
    "    X_test_reduced = X_test[selected_features]\n",
    "    \n",
    "    return(X_train_reduced, X_test_reduced, selected_features)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs nested cross validation, optimizing the model at each step in order to properly estimate the model's error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV(outer_cv, inner_cv,\n",
    "               X_train, y_train, \n",
    "               model, search_space, \n",
    "               n_jobs, n_iter_bsearch, verbose):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function performs nested cross validation to estimate model's error rate\n",
    "    It take the cv number, the training dataset, the target variable, the model or a BayesSearchCV\n",
    "    and a search space to perform hyperparameter tuning\n",
    "    \"\"\"\n",
    "\n",
    "    kf = sk.model_selection.KFold(n_splits = outer_cv, \n",
    "                                  shuffle = True, \n",
    "                                  random_state = kfold_random_state)\n",
    "    \n",
    "    accuracy_array = np.zeros(outer_cv)\n",
    "    f1_array = np.zeros(outer_cv)\n",
    "    precision_array = np.zeros(outer_cv)\n",
    "    recall_array = np.zeros(outer_cv)\n",
    "    roc_auc_array = np.zeros(outer_cv)\n",
    "    predictions_array = np.zeros(len(X_train))\n",
    "    probas_array = np.zeros(len(X_train))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold #{i + 1}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 1 (Splitting)\")\n",
    "        \n",
    "        X_train_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_cv, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 2 (Tuning)\")\n",
    "\n",
    "        opti = BayesSearchCV(\n",
    "            estimator = model,\n",
    "            search_spaces = search_space,\n",
    "            n_iter = n_iter_bsearch,\n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True,\n",
    "            n_jobs = n_jobs,\n",
    "            verbose = False\n",
    "        )\n",
    "\n",
    "        opti.fit(X_train_cv, y_train_cv)\n",
    "        best_model = opti.best_estimator_\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"STEP 3 (Prediction)\")\n",
    "\n",
    "        cv_predictions = best_model.predict(X_val)\n",
    "        cv_pred_probas = best_model.predict_proba(X_val)[:,1]\n",
    "        \n",
    "        cv_accuracy = sk.metrics.accuracy_score(y_val, cv_predictions)\n",
    "        cv_f1 = sk.metrics.f1_score(y_val, cv_predictions)\n",
    "        cv_precision = sk.metrics.precision_score(y_val, cv_predictions)\n",
    "        cv_recall = sk.metrics.recall_score(y_val, cv_predictions)\n",
    "        cv_roc_auc = sk.metrics.roc_auc_score(y_val, cv_pred_probas)\n",
    "                \n",
    "        accuracy_array[i] = cv_accuracy \n",
    "        f1_array[i] = cv_f1\n",
    "        precision_array[i] = cv_precision\n",
    "        recall_array[i] = cv_recall\n",
    "        roc_auc_array[i] = cv_roc_auc\n",
    "        predictions_array[val_index] = cv_predictions\n",
    "        probas_array[val_index] = cv_pred_probas\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(accuracy_array, f1_array, precision_array, recall_array, roc_auc_array, predictions_array, probas_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV la vendetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV_revenge(outer_cv, inner_cv,\n",
    "                      X_train, y_train, \n",
    "                      model, search_space, \n",
    "                      n_jobs, n_iter_bsearch, verbose):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function performs nested cross validation to estimate model's error rate\n",
    "    It take the cv number, the training dataset, the target variable, the model or a BayesSearchCV\n",
    "    and a search space to perform hyperparameter tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = sk.model_selection.KFold(n_splits = outer_cv, \n",
    "                                  shuffle=True, \n",
    "                                  random_state = kfold_random_state)\n",
    "    \n",
    "\n",
    "    hyperparam_list = []\n",
    "    selected_features_list = []\n",
    "    accuracy_array = np.zeros(outer_cv)\n",
    "    f1_array = np.zeros(outer_cv)\n",
    "    precision_array = np.zeros(outer_cv)\n",
    "    recall_array = np.zeros(outer_cv)\n",
    "    roc_auc_array = np.zeros(outer_cv)\n",
    "    predictions_array = np.zeros(len(X_train))\n",
    "    probas_array = np.zeros(len(X_train))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold #{i + 1}\")\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"STEP 1 (Splitting)\")\n",
    "            \n",
    "        X_train_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_cv, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 2 (Tuning)\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.1 (HyperParameter Tuning Part 1)\")\n",
    "        \n",
    "        opti = BayesSearchCV(\n",
    "            estimator = model,\n",
    "            search_spaces = search_space,\n",
    "            n_iter = n_iter_bsearch,\n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True,\n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        opti.fit(X_train_cv, y_train_cv)\n",
    "        best_model = opti.best_estimator_\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.2 (SHAP)\")\n",
    "        \n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "        feature_names = best_model.feature_names_in_\n",
    "        feature_importances_shap = np.mean(np.abs(explainer.shap_values(X_train_cv)), axis = 0)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"STEP 2.3 (Feature selection)\")\n",
    "        \n",
    "        X_train_cv_reduced, X_val_cv_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                                        best_model, \n",
    "                                                                                        X_train_cv, X_val, y_train_cv,\n",
    "                                                                                        inner_cv,\n",
    "                                                                                        plot = False, verbose = False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.4 (HyperParameter Tuning Part 2)\")\n",
    "        \n",
    "        opti_fs = BayesSearchCV(\n",
    "            estimator = best_model, \n",
    "            search_spaces = search_space, \n",
    "            n_iter = n_iter_bsearch, \n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True, \n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        opti_fs.fit(X_train_cv_reduced, y_train_cv)\n",
    "        best_model_fs = opti_fs.best_estimator_\n",
    "        best_params_fs = opti_fs.best_params_\n",
    "\n",
    "        if verbose: \n",
    "            print(\"STEP 3 (Threshold Tuning)\")\n",
    "\n",
    "        tuner_thr = sk.model_selection.TunedThresholdClassifierCV(best_model_fs, \n",
    "                                                                  scoring = \"balanced_accuracy\", \n",
    "                                                                  cv = inner_cv)\n",
    "        \n",
    "        model_thr = tuner_thr.fit(X_train_cv_reduced, y_train_cv)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 4 (Prediction)\")\n",
    "        \n",
    "        cv_predictions = model_thr.predict(X_val_cv_reduced)\n",
    "        cv_pred_probas = model_thr.predict_proba(X_val_cv_reduced)[:,1]\n",
    "        \n",
    "        cv_accuracy = sk.metrics.accuracy_score(y_val, cv_predictions)\n",
    "        cv_f1 = sk.metrics.f1_score(y_val, cv_predictions)\n",
    "        cv_precision = sk.metrics.precision_score(y_val, cv_predictions)\n",
    "        cv_recall = sk.metrics.recall_score(y_val, cv_predictions)\n",
    "        cv_roc_auc = sk.metrics.roc_auc_score(y_val, cv_pred_probas)\n",
    "        \n",
    "        hyperparam_list.append(best_params_fs)\n",
    "        selected_features_list.append(selected_features)\n",
    "        accuracy_array[i] = cv_accuracy \n",
    "        f1_array[i] = cv_f1\n",
    "        precision_array[i] = cv_precision\n",
    "        recall_array[i] = cv_recall\n",
    "        roc_auc_array[i] = cv_roc_auc\n",
    "        predictions_array[val_index] = cv_predictions\n",
    "        probas_array[val_index] = cv_pred_probas\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(hyperparam_list, selected_features_list, \n",
    "           accuracy_array, f1_array, precision_array, \n",
    "           recall_array, roc_auc_array, predictions_array, probas_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = load_datasets_from_r(path_to_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = output_dict[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 5))\n",
    "\n",
    "A = axes[0,0]\n",
    "B = axes[0,1]\n",
    "C = axes[1,0]\n",
    "D = axes[1,1]\n",
    "\n",
    "sns.barplot(working_df.bool_diff_acc.value_counts(), ax = A)\n",
    "sns.barplot(working_df.sign_mean_log2FC_1.value_counts(), ax = B)\n",
    "sns.barplot(working_df.sign_mean_log2FC_2.value_counts(), ax = C)\n",
    "sns.barplot(working_df.sign_mean_log2FC_3.value_counts(), ax = D)\n",
    "\n",
    "A.set_title(\"Differential Accessibility across patients (boolean) [A]\")\n",
    "B.set_title(\"Differential Accessibility direction (v = 1) [B]\")\n",
    "C.set_title(\"Differential Accessibility direction (v = 2) [C]\")\n",
    "D.set_title(\"Differential Accessibility direction (v = 3) [D]\")\n",
    "\n",
    "A.set_xlabel(\"\"); A.set_ylabel(\"\")\n",
    "B.set_xlabel(\"\"); B.set_ylabel(\"\")\n",
    "C.set_xlabel(\"\"); C.set_ylabel(\"\")\n",
    "D.set_xlabel(\"\"); D.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ids = working_df[[\"bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df.drop(cols_to_drop, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = working_df.iloc[:, 1:]\n",
    "y = working_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sign_mean_log2FC_1\" in X.columns:\n",
    "    raise Exception(\"Target Variable in the training set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, bin_train, bin_test = sk.model_selection.train_test_split(X, y, bin_ids, test_size=0.3, random_state=split_random_state)\n",
    "y_train = np.where(y_train == 1, 1, 0); y_test = np.where(y_test == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Classifier and the Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = xgb.XGBClassifier(n_jobs = n_jobs, nthread = nthread, seed = classifier_seed, random_state = classifier_random_state)\n",
    "\n",
    "search_space_bayes = {  \n",
    "    'learning_rate': (0.001, 1.0, 'log-uniform'),  \n",
    "    'n_estimators': (50, 500),  \n",
    "    'max_depth': (3, 15),  \n",
    "    'min_child_weight': (1, 10),  \n",
    "    'gamma': (1e-9, 5.0, 'log-uniform'),  \n",
    "    'subsample': (0.5, 1.0, 'uniform'),  \n",
    "    'colsample_bytree': (0.5, 1.0, 'uniform'),  \n",
    "    'colsample_bylevel': (0.5, 1.0, 'uniform'),  \n",
    "    'reg_alpha': (1e-9, 10.0, 'log-uniform'),  \n",
    "    'reg_lambda': (1e-9, 10.0, 'log-uniform'),  \n",
    "    'scale_pos_weight': (1e-6, 100, 'log-uniform'),  \n",
    "    'max_delta_step': (0, 10),  \n",
    "    'objective': ['binary:logistic'],  \n",
    "    'booster': ['gbtree', 'dart'],  \n",
    "    'eval_metric': ['logloss', 'error', 'auc']  \n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NESTED CV PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2.2 (SHAP)\n",
      "STEP 2.3 (Feature selection)\n",
      "STEP 2.4 (HyperParameter Tuning Part 2)\n",
      "STEP 3 (Threshold Tuning)\n",
      "STEP 4 (Prediction)\n",
      "Fold #2\n",
      "STEP 1 (Splitting)\n",
      "STEP 2 (Tuning)\n",
      "STEP 2.1 (HyperParameter Tuning Part 1)\n",
      "STEP 2.2 (SHAP)\n",
      "STEP 2.3 (Feature selection)\n",
      "STEP 2.4 (HyperParameter Tuning Part 2)\n",
      "STEP 3 (Threshold Tuning)\n",
      "STEP 4 (Prediction)\n",
      "Fold #3\n",
      "STEP 1 (Splitting)\n",
      "STEP 2 (Tuning)\n",
      "STEP 2.1 (HyperParameter Tuning Part 1)\n",
      "STEP 2.2 (SHAP)\n",
      "STEP 2.3 (Feature selection)\n",
      "STEP 2.4 (HyperParameter Tuning Part 2)\n",
      "STEP 3 (Threshold Tuning)\n",
      "STEP 4 (Prediction)\n",
      "Fold #4\n",
      "STEP 1 (Splitting)\n",
      "STEP 2 (Tuning)\n",
      "STEP 2.1 (HyperParameter Tuning Part 1)\n",
      "STEP 2.2 (SHAP)\n",
      "STEP 2.3 (Feature selection)\n"
     ]
    }
   ],
   "source": [
    "results = nested_CV_revenge(\n",
    "    outer_cv, inner_cv,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model=classifier,\n",
    "    search_space=search_space_bayes,\n",
    "    n_jobs= n_jobs, n_iter_bsearch = n_iter_bsearch,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "(\n",
    "    hyperparam_list,\n",
    "    selected_features_list,\n",
    "    accuracy_array,\n",
    "    f1_array,\n",
    "    precision_array,\n",
    "    recall_array,\n",
    "    roc_auc_array,\n",
    "    predictions_array,\n",
    "    probas_array\n",
    ") = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The cross-validated accuracy (cv = {outer_cv}) is: {np.mean(accuracy_array)}\")\n",
    "print(f\"The cross-validated f1 (cv = {outer_cv}) is: {np.mean(f1_array)}\")\n",
    "print(f\"The cross-validated precision (cv = {outer_cv}) is: {np.mean(precision_array)}\")\n",
    "print(f\"The cross-validated R recall (cv = {outer_cv}) is: {np.mean(recall_array)}\")\n",
    "print(f\"The cross-validated AUC (cv = {outer_cv}) is: {np.mean(roc_auc_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_in_at_least_one_cv = set([elem for inner_list in selected_features_list for elem in inner_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_grid = {}\n",
    "\n",
    "for dictionary in hyperparam_list:\n",
    "    for k,v in dictionary.items():\n",
    "        if k in search_space_grid:\n",
    "            search_space_grid[k].update([v])\n",
    "        else:\n",
    "            search_space_grid[k] = set([v])\n",
    "\n",
    "search_space_grid = {k : list(v) for k,v in search_space_grid.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = sk.model_selection.RandomizedSearchCV(estimator = classifier, \n",
    "                                             param_distributions = search_space_grid, \n",
    "                                             cv = inner_cv, \n",
    "                                             n_jobs = n_jobs,\n",
    "                                             n_iter = n_iter_rsearch,\n",
    "                                             random_state = hypertune_random_state_rsearch, \n",
    "                                             verbose = 0, refit = True)\n",
    "opti.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = opti.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at both XGBoost native feature importance and at mean absolute SHAP values as metric of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_train)\n",
    "shap_values_numpy = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_shap:\n",
    "    print(\" / \".join([mode, condition]))\n",
    "    plt.figure(figsize = (100,25))\n",
    "    summary_plot = shap.summary_plot(shap_values, X_train, \n",
    "                                     plot_type='bar', max_display=10, plot_size= (25,10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform Feature Selection using mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model.feature_names_in_\n",
    "feature_importances_shap = np.mean(np.abs(explainer.shap_values(X_train)), axis = 0)\n",
    "\n",
    "X_train_reduced, X_test_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                            model, X_train, X_test, y_train, \n",
    "                                                                            inner_cv = inner_cv,\n",
    "                                                                            plot = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_discard = [feature for feature in selected_features if feature not in selected_features_in_at_least_one_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dropping columns never selected during Nested Cross Validation: {features_to_discard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train_reduced.drop(features_to_discard, axis = 1)\n",
    "X_test_reduced = X_test_reduced.drop(features_to_discard, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti_fs = sk.model_selection.RandomizedSearchCV(estimator = model, \n",
    "                                                param_distributions = search_space_grid, \n",
    "                                                cv = inner_cv, \n",
    "                                                n_jobs = n_jobs,\n",
    "                                                n_iter = n_iter_rsearch,\n",
    "                                                random_state = hypertune_random_state_rsearch, \n",
    "                                                verbose = 0, refit = True)\n",
    "opti_fs.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fs = opti_fs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_fs = opti_fs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in best_params_fs.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_thr = sk.model_selection.TunedThresholdClassifierCV(model_fs, \n",
    "                                                          scoring = \"balanced_accuracy\", \n",
    "                                                          cv = inner_cv)\n",
    "model_thr = tuner_thr.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thr = model_thr.best_threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tuned Classification Threshold is: {best_thr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_array_thr = np.mean(sk.model_selection.cross_val_score(model_thr, X_train_reduced, y_train, cv = outer_cv, scoring = \"accuracy\"))\n",
    "f1_array_thr = np.mean(sk.model_selection.cross_val_score(model_thr, X_train_reduced, y_train, cv = outer_cv, scoring = \"f1\"))\n",
    "precision_array_thr = np.mean(sk.model_selection.cross_val_score(model_thr, X_train_reduced, y_train, cv = outer_cv, scoring = \"precision\"))\n",
    "recall_array_thr = np.mean(sk.model_selection.cross_val_score(model_thr, X_train_reduced, y_train, cv = outer_cv, scoring = \"recall\"))\n",
    "roc_auc_array_thr = np.mean(sk.model_selection.cross_val_score(model_thr, X_train_reduced, y_train, cv = outer_cv, scoring = \"roc_auc\"))\n",
    "proba_predictions_thr = sk.model_selection.cross_val_predict(model_thr, X_train_reduced, y_train, cv = outer_cv, method= \"predict_proba\")[:,1]\n",
    "class_array_thr = sk.model_selection.cross_val_predict(model_thr, X_train_reduced, y_train, cv = outer_cv, method= \"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The cross-validated accuracy (cv = {outer_cv}) is: {np.mean(accuracy_array_thr)}\")\n",
    "print(f\"The cross-validated f1 score(cv = {outer_cv}) is: {np.mean(f1_array_thr)}\")\n",
    "print(f\"The cross-validated precision (cv = {outer_cv}) is: {np.mean(precision_array_thr)}\")\n",
    "print(f\"The cross-validated recall(cv = {outer_cv}) is: {np.mean(recall_array_thr)}\")\n",
    "print(f\"The cross-validated roc auc (cv = {outer_cv}) is: {np.mean(roc_auc_array_thr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sk.metrics.roc_curve(y_train, proba_predictions_thr)\n",
    "roc_auc = sk.metrics.auc(fpr, tpr)\n",
    "\n",
    "cm = sk.metrics.confusion_matrix(y_train, class_array_thr)\n",
    "disp = sk.metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {np.mean(roc_auc):.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.0])\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "disp.plot(ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Variables for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_train_reduced,open(X_train_filename,\"wb\"))\n",
    "pickle.dump(X_test_reduced,open(X_test_filename,\"wb\"))\n",
    "pickle.dump(y_train,open(y_train_filename,\"wb\"))\n",
    "pickle.dump(y_test,open(y_test_filename,\"wb\"))\n",
    "pickle.dump(bin_train,open(bin_train_filename,\"wb\"))\n",
    "pickle.dump(bin_test,open(bin_test_filename,\"wb\"))\n",
    "pickle.dump(model_thr,open(model_filename,\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (System)",
   "language": "python",
   "name": "python3-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
