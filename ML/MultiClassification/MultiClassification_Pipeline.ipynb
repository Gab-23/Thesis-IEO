{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the python kernel we are using, should be the local one, not the one in the virtual environment. <br>\n",
    "NOTE: A proper virtual environment will be setup later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up main variables that will be called later on, for readibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_bool = False\n",
    "path_to_datasets = \"/home/ieo7429/Desktop/THESIS_GAB/outfiles/target_variables/bulk_ML_tables_regression_classification_BRCA_0.1Mbp_with_HIC_and_Repliseq.RData\"\n",
    "\n",
    "mode = \"regrout\"\n",
    "technology = \"bulk\"\n",
    "\n",
    "if technology == \"sc\":\n",
    "    \n",
    "    cols_to_drop = [\"mean_log2FC\", \"joint_probability\", \"bool_diff_acc\", \n",
    "                    \"sign_mean_log2FC_1\", \"sign_mean_log2FC_2\", \"sign_mean_log2FC_3\", \n",
    "                    \"weighted_coef_of_var_log2FC\", \"weighted_log2FC\", \"bin\", \"Type\", \"replication_class\"]\n",
    "\n",
    "    cols_to_check = [\"mean_log2FC\", \"weighted_log2FC\", \"weighted_coef_of_var_log2FC\",\n",
    "                 \"joint_probability\", \"bool_diff_acc\", \"sign_mean_log2FC_1\", \n",
    "                 \"sign_mean_log2FC_2\", \"sign_mean_log2FC_3\", \"joined_direction\", \"significant_dir_1\"]\n",
    "\n",
    "else:\n",
    "    \n",
    "    cols_to_drop = [\"bool_diff_acc\", \"logFC\", \"significant_dir_1\",\n",
    "                    \"sign_mean_log2FC_2\", \"sign_mean_log2FC_3\", \n",
    "                    \"bin\", \"Type\", \"replication_class\"]\n",
    "\n",
    "    cols_to_check = [\"logFC\", \"bool_diff_acc\", \"sign_mean_log2FC_1\", \n",
    "                     \"sign_mean_log2FC_2\", \"sign_mean_log2FC_3\", \"joined_direction\",\n",
    "                     \"significant_dir_1\"]\n",
    "\n",
    "if \"ampl_score\" in cols_to_drop and \"del_score\" in cols_to_drop:\n",
    "\n",
    "    condition = \"without_CNA\"\n",
    "\n",
    "else:\n",
    "\n",
    "    condition = \"with CNA\"\n",
    "\n",
    "n_jobs_boost = 60\n",
    "n_jobs_sk = 1\n",
    "n_iter_bsearch = 25\n",
    "n_iter_rsearch = 50\n",
    "\n",
    "split_random_state = 489574\n",
    "classifier_seed = 3737\n",
    "classifier_random_state = 39473209\n",
    "hypertune_random_state_bsearch = 3847\n",
    "hypertune_random_state_rsearch = 49574\n",
    "kfold_random_state = 4909\n",
    "\n",
    "outer_cv = 5\n",
    "inner_cv = 3\n",
    "plot_variation = True\n",
    "plot_shap = True\n",
    "\n",
    "import os\n",
    "\n",
    "X_train_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_X_train_red\"; \n",
    "X_test_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_X_test_red\"\n",
    "y_train_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_y_train_red\"; \n",
    "y_test_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_y_test_red\"\n",
    "bin_train_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_bin_train_red\"; \n",
    "bin_test_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_bin_test_red\"\n",
    "model_filename = \"bulk_MultiClassification_Output_with_HIC_and_Repliseq_0.1Mbp/multiclass_final_model\"\n",
    "\n",
    "file_paths = [\n",
    "    X_train_filename, X_test_filename,\n",
    "    y_train_filename, y_test_filename,\n",
    "    bin_train_filename, bin_test_filename,\n",
    "    model_filename\n",
    "]\n",
    "\n",
    "base_dirs = {os.path.dirname(path) for path in file_paths}\n",
    "\n",
    "if len(base_dirs) != 1:\n",
    "    raise ValueError(f\"Inconsistent base directories found: {base_dirs}\")\n",
    "\n",
    "base_dir = base_dirs.pop()\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    raise FileNotFoundError(f\"Base directory does not exist: {base_dir}\")\n",
    "\n",
    "print(f\"Base directory is consistent and exists: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if install_bool:\n",
    "    ! pip install pickleshare\n",
    "    ! pip install pandas\n",
    "    ! pip install scikit-learn\n",
    "    ! pip install seaborn\n",
    "    ! pip install scipy\n",
    "    ! pip install --upgrade pip setuptools wheel\n",
    "    ! pip install rpy2\n",
    "    ! pip install shap\n",
    "    ! pip install \"numpy<=2.1\"\n",
    "    %pip install -U ipywidgets\n",
    "\n",
    "clear_output(wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import rpy2.robjects as ro\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings\n",
    "import pickleshare\n",
    "import pickle\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### load_datasets_from_r\n",
    "Reads \"ML.Tables\" .Rdata file and converts it to a nested dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_from_r(file_path):\n",
    "\n",
    "    \"\"\" \n",
    "    The function uses rpy2 to load datasets from an RData file.\n",
    "    In particular it expects a list of data.frames\n",
    "    It returns a dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    pandas2ri.activate()\n",
    "\n",
    "    ro.r['load'](file_path) # load the .RData\n",
    "\n",
    "    env = ro.globalenv # set the environment\n",
    "\n",
    "    r_list = env[list(env.keys())[0]] # take the list object\n",
    "\n",
    "    list_names = ro.r['names'](r_list) # take the names\n",
    "    list_names = [str(name) for name in list_names]\n",
    "\n",
    "    df_dict = {} # initialize outer dictionary\n",
    "    with (ro.default_converter + ro.pandas2ri.converter).context(): # start the conversion\n",
    "    # the conversion transforms a list of lists of lists of data.frames \n",
    "    # to a list of dictionaries of dictionaries of dictionaries of DataFrame\n",
    "        i = 0\n",
    "        for outer_dict in r_list:\n",
    "            df_dict[list_names[i]] = outer_dict\n",
    "            i += 1\n",
    "            \n",
    "    return(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### gini_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute the gini coefficient (2 * AUC - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient_score(y_true, y_pred_prob, **kwargs):\n",
    "    y_score = y_pred_prob\n",
    "    auc_score = sk.metrics.roc_auc_score(y_true, y_score, multi_class='ovr')\n",
    "    gini_coef = 2 * auc_score - 1\n",
    "    return gini_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_coefficient = sk.metrics.make_scorer(gini_coefficient_score, response_method = \"predict_proba\", greater_is_better= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### feature_selector\n",
    "his function performs Incremental Feature Selection (IFS) to produce a reduced model <br> with best gini coefficient (2 * AUC - 1) performance with cross validation.\n",
    "Mean absolute SHAP values are used to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector_shap(feature_names, feature_importances, model, \n",
    "                          X_train, X_test, y_train, inner_cv,\n",
    "                          plot = True, verbose = True):\n",
    "\n",
    "    \"\"\"\n",
    "    The function takes as input feature names, feature importances, \n",
    "    a XGBoost Classifier object, the training and test dataset\n",
    "    and returns the reduced training and test datasets.\n",
    "    - plot parameter regulates whether or not to plot the Gini Coefficient vs number of feature plot\n",
    "    - verbose parameter regulates whether or not to produce a text output\n",
    "    \"\"\"\n",
    "\n",
    "    thresholds = np.sort(feature_importances) # sort the feature importances\n",
    "    num_features_list = [] # initialize list\n",
    "    gini_coef_list = [] # initialize list\n",
    "    \n",
    "    for threshold in thresholds: # iterate over thresholds\n",
    "        \n",
    "        vars_to_keep = np.where(feature_importances >= threshold)[0]\n",
    "        X_train_selected = X_train.iloc[:,vars_to_keep]\n",
    "\n",
    "        gini_coef_scores = sk.model_selection.cross_val_score(estimator = model, \n",
    "                                                              X = X_train_selected, y = y_train, \n",
    "                                                              scoring = gini_coefficient, cv = inner_cv)\n",
    "        mean_gini_coef_score = np.mean(gini_coef_scores)\n",
    "        \n",
    "        num_features_list.append(X_train_selected.shape[1]) # append\n",
    "        gini_coef_list.append(mean_gini_coef_score) # append\n",
    "\n",
    "        if verbose:\n",
    "            print(f'> threshold={threshold}, features={X_train_selected.shape[1]}, gini coefficient={mean_gini_coef_score}')\n",
    "        \n",
    "        if len(vars_to_keep) == 1:\n",
    "            break\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(num_features_list, gini_coef_list, marker='o')\n",
    "        plt.xlabel('Number of Selected Features')\n",
    "        plt.ylabel('cross validated mean Ginny coefficient')\n",
    "        plt.title('Gini coefficient vs. Number of Selected Features')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    optimal_threshold_index = np.argmax(gini_coef_list)\n",
    "    optimal_num_features = num_features_list[optimal_threshold_index]\n",
    "    \n",
    "    if (optimal_threshold_index) == 0:\n",
    "        optimal_threshold = 0\n",
    "    else:   \n",
    "        optimal_threshold = thresholds[optimal_threshold_index - 1]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"Number of Selected Features: {optimal_num_features}\")\n",
    "        print(f\"Gini coef at Optimal Threshold: {gini_coef_list[optimal_threshold_index]:.4f}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    n = 5\n",
    "    selected_features = feature_names[np.where(feature_importances > optimal_threshold)]\n",
    "    selected_features = [str(name) for name in selected_features]\n",
    "    discarded_features = [str(name) for name in feature_names if str(name) not in selected_features]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Selected features are: \\n\")\n",
    "        for i in range(0,len(selected_features), n):\n",
    "            print(\"  \".join(selected_features[i:i+n]))\n",
    "        print(\"\\n\")\n",
    "        print(\"Discarded features are: \\n\")\n",
    "        for i in range(0,len(discarded_features), n):\n",
    "            print(\"  \".join(discarded_features[i:i+n]))\n",
    "\n",
    "    X_train_reduced = X_train[selected_features]\n",
    "    X_test_reduced = X_test[selected_features]\n",
    "    \n",
    "    return(X_train_reduced, X_test_reduced, selected_features)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV la vendetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV_revenge(outer_cv, inner_cv,\n",
    "                      X_train, y_train, \n",
    "                      model, search_space, \n",
    "                      n_jobs, n_iter_bsearch, verbose):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function performs nested cross validation to estimate model's error rate\n",
    "    It take the cv number, the training dataset, the target variable, the model or a BayesSearchCV\n",
    "    and a search space to perform hyperparameter tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = sk.model_selection.KFold(n_splits = outer_cv, \n",
    "                                  shuffle=True, \n",
    "                                  random_state = kfold_random_state)\n",
    "    \n",
    "\n",
    "    hyperparam_list = []\n",
    "    selected_features_list = []\n",
    "    accuracy_array = np.zeros(outer_cv)\n",
    "    f1_array = np.zeros(outer_cv)\n",
    "    precision_array = np.zeros(outer_cv)\n",
    "    recall_array = np.zeros(outer_cv)\n",
    "    roc_auc_array = np.zeros(outer_cv)\n",
    "    predictions_array = np.zeros(len(X_train))\n",
    "    probas_array = np.zeros((len(X_train),len(np.unique(y_train))))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold #{i + 1}\")\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"STEP 1 (Splitting)\")\n",
    "            \n",
    "        X_train_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 2 (Tuning)\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.1 (HyperParameter Tuning Part 1)\")\n",
    "        \n",
    "        opti = BayesSearchCV(\n",
    "            estimator = model,\n",
    "            search_spaces = search_space,\n",
    "            n_iter = n_iter_bsearch,\n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True,\n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 3\n",
    "        )\n",
    "        \n",
    "        opti.fit(X_train_cv, y_train_cv)\n",
    "        best_model = opti.best_estimator_\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.2 (SHAP)\")\n",
    "        \n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "        feature_names = best_model.feature_names_in_\n",
    "        feature_importances_shap = np.mean(np.mean(np.abs(explainer.shap_values(X_train_cv)), axis = 0), axis = 1)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"STEP 2.3 (Feature selection)\")\n",
    "        \n",
    "        X_train_cv_reduced, X_val_cv_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                                        best_model, \n",
    "                                                                                        X_train_cv, X_val, y_train_cv,\n",
    "                                                                                        inner_cv,\n",
    "                                                                                        plot = False, verbose = False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.4 (HyperParameter Tuning Part 2)\")\n",
    "        \n",
    "        opti_fs = BayesSearchCV(\n",
    "            estimator = best_model, \n",
    "            search_spaces = search_space, \n",
    "            n_iter = n_iter_bsearch, \n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True, \n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 3\n",
    "        )\n",
    "        \n",
    "        opti_fs.fit(X_train_cv_reduced, y_train_cv)\n",
    "        best_model_fs = opti_fs.best_estimator_\n",
    "        best_params_fs = opti_fs.best_params_\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 3 (Prediction)\")\n",
    "        \n",
    "        cv_predictions = best_model_fs.predict(X_val_cv_reduced)\n",
    "        cv_pred_probas = best_model_fs.predict_proba(X_val_cv_reduced)\n",
    "        \n",
    "        cv_accuracy = sk.metrics.accuracy_score(y_val, cv_predictions)\n",
    "        cv_f1 = sk.metrics.f1_score(y_val, cv_predictions, average = \"macro\")\n",
    "        cv_precision = sk.metrics.precision_score(y_val, cv_predictions, average = \"macro\")\n",
    "        cv_recall = sk.metrics.recall_score(y_val, cv_predictions, average = \"macro\")\n",
    "        cv_roc_auc = sk.metrics.roc_auc_score(y_val, cv_pred_probas, multi_class='ovr', average = \"macro\")\n",
    "        \n",
    "        hyperparam_list.append(best_params_fs)\n",
    "        selected_features_list.append(selected_features)\n",
    "        accuracy_array[i] = cv_accuracy \n",
    "        f1_array[i] = cv_f1\n",
    "        precision_array[i] = cv_precision\n",
    "        recall_array[i] = cv_recall\n",
    "        roc_auc_array[i] = cv_roc_auc\n",
    "        predictions_array[val_index] = cv_predictions\n",
    "        probas_array[val_index] = cv_pred_probas\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(hyperparam_list, selected_features_list, \n",
    "           accuracy_array, f1_array, precision_array, \n",
    "           recall_array, roc_auc_array, predictions_array, probas_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV_revenge_binary(outer_cv, inner_cv,\n",
    "                      X_train, y_train, \n",
    "                      model, search_space, \n",
    "                      n_jobs, n_iter_bsearch, verbose):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function performs nested cross validation to estimate model's error rate\n",
    "    It take the cv number, the training dataset, the target variable, the model or a BayesSearchCV\n",
    "    and a search space to perform hyperparameter tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = sk.model_selection.KFold(n_splits = outer_cv, \n",
    "                                  shuffle=True, \n",
    "                                  random_state = kfold_random_state)\n",
    "    \n",
    "\n",
    "    hyperparam_list = []\n",
    "    selected_features_list = []\n",
    "    accuracy_array = np.zeros(outer_cv)\n",
    "    f1_array = np.zeros(outer_cv)\n",
    "    precision_array = np.zeros(outer_cv)\n",
    "    recall_array = np.zeros(outer_cv)\n",
    "    roc_auc_array = np.zeros(outer_cv)\n",
    "    predictions_array = np.zeros(len(X_train))\n",
    "    probas_array = np.zeros((len(X_train),len(np.unique(y_train))))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold #{i + 1}\")\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"STEP 1 (Splitting)\")\n",
    "            \n",
    "        X_train_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 2 (Tuning)\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.1 (HyperParameter Tuning Part 1)\")\n",
    "        \n",
    "        opti = BayesSearchCV(\n",
    "            estimator = model,\n",
    "            search_spaces = search_space,\n",
    "            n_iter = n_iter_bsearch,\n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True,\n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 3\n",
    "        )\n",
    "        \n",
    "        opti.fit(X_train_cv, y_train_cv)\n",
    "        best_model = opti.best_estimator_\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.2 (SHAP)\")\n",
    "        \n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "        feature_names = best_model.feature_names_in_\n",
    "        feature_importances_shap = np.mean(np.abs(explainer.shap_values(X_train_cv)), axis = 0)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"STEP 2.3 (Feature selection)\")\n",
    "        \n",
    "        X_train_cv_reduced, X_val_cv_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                                        best_model, \n",
    "                                                                                        X_train_cv, X_val, y_train_cv,\n",
    "                                                                                        inner_cv,\n",
    "                                                                                        plot = False, verbose = False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.4 (HyperParameter Tuning Part 2)\")\n",
    "        \n",
    "        opti_fs = BayesSearchCV(\n",
    "            estimator = best_model, \n",
    "            search_spaces = search_space, \n",
    "            n_iter = n_iter_bsearch, \n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True, \n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 3\n",
    "        )\n",
    "        \n",
    "        opti_fs.fit(X_train_cv_reduced, y_train_cv)\n",
    "        best_model_fs = opti_fs.best_estimator_\n",
    "        best_params_fs = opti_fs.best_params_\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 3 (Prediction)\")\n",
    "        \n",
    "        cv_predictions = best_model_fs.predict(X_val_cv_reduced)\n",
    "        cv_pred_probas = best_model_fs.predict_proba(X_val_cv_reduced)\n",
    "        \n",
    "        cv_accuracy = sk.metrics.accuracy_score(y_val, cv_predictions)\n",
    "        cv_f1 = sk.metrics.f1_score(y_val, cv_predictions)\n",
    "        cv_precision = sk.metrics.precision_score(y_val, cv_predictions)\n",
    "        cv_recall = sk.metrics.recall_score(y_val, cv_predictions)\n",
    "        cv_roc_auc = sk.metrics.roc_auc_score(y_val, cv_pred_probas)\n",
    "        \n",
    "        hyperparam_list.append(best_params_fs)\n",
    "        selected_features_list.append(selected_features)\n",
    "        accuracy_array[i] = cv_accuracy \n",
    "        f1_array[i] = cv_f1\n",
    "        precision_array[i] = cv_precision\n",
    "        recall_array[i] = cv_recall\n",
    "        roc_auc_array[i] = cv_roc_auc\n",
    "        predictions_array[val_index] = cv_predictions\n",
    "        probas_array[val_index] = cv_pred_probas\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(hyperparam_list, selected_features_list, \n",
    "           accuracy_array, f1_array, precision_array, \n",
    "           recall_array, roc_auc_array, predictions_array, probas_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = load_datasets_from_r(path_to_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = output_dict[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_dir_1 = np.where(\n",
    "    (working_df.bool_diff_acc == 1) & (working_df.sign_mean_log2FC_1 == 1), 2,\n",
    "    np.where(\n",
    "        (working_df.bool_diff_acc == 1) & (working_df.sign_mean_log2FC_1 == -1), 0, 1\n",
    "    )\n",
    ")\n",
    "\n",
    "sign_mean_log2FC_1 = np.where(working_df.sign_mean_log2FC_1 == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[\"significant_dir_1\"] = significant_dir_1\n",
    "working_df[\"sign_mean_log2FC_1\"] = sign_mean_log2FC_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(12, 5))\n",
    "\n",
    "A = axes[0,0]\n",
    "B = axes[0,1]\n",
    "C = axes[1,0]\n",
    "D = axes[1,1]\n",
    "E = axes[2,0]\n",
    "F = axes[2,1]\n",
    "\n",
    "sns.barplot(working_df.bool_diff_acc.value_counts(), ax = A)\n",
    "sns.barplot(working_df.sign_mean_log2FC_1.value_counts(), ax = B)\n",
    "sns.barplot(working_df.sign_mean_log2FC_2.value_counts(), ax = C)\n",
    "sns.barplot(working_df.sign_mean_log2FC_3.value_counts(), ax = D)\n",
    "sns.barplot(working_df.significant_dir_1.value_counts(), ax = E)\n",
    "\n",
    "\n",
    "A.set_title(\"Differential Accessibility across patients (boolean) [A]\")\n",
    "B.set_title(\"Differential Accessibility direction (v = 1) [B]\")\n",
    "C.set_title(\"Differential Accessibility direction (v = 2) [C]\")\n",
    "D.set_title(\"Differential Accessibility direction (v = 3) [D]\")\n",
    "E.set_title(\"Signficant Direction (v = 1) [E]\")\n",
    "\n",
    "A.set_xlabel(\"\"); A.set_ylabel(\"\")\n",
    "B.set_xlabel(\"\"); B.set_ylabel(\"\")\n",
    "C.set_xlabel(\"\"); C.set_ylabel(\"\")\n",
    "D.set_xlabel(\"\"); D.set_ylabel(\"\")\n",
    "E.set_xlabel(\"\"); E.set_ylabel(\"\")\n",
    "\n",
    "F.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ids = working_df[[\"bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df.drop(cols_to_drop, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if technology == \"sc\":\n",
    "    X = working_df.iloc[:, 0:-1]\n",
    "    y = working_df.iloc[:, -1]\n",
    "else:\n",
    "    X = working_df.iloc[:,1:]\n",
    "    y = working_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len([col for col in cols_to_check if col in X.columns]) > 0:\n",
    "    raise Exception(\"Target Variable and derivated in the training set!\")\n",
    "else:\n",
    "    print(\"Target Variable outside training set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, bin_train, bin_test = sk.model_selection.train_test_split(X, y, bin_ids, test_size=0.3, random_state=split_random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Classifier and the Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = xgb.XGBClassifier(n_jobs = n_jobs_boost, seed = classifier_seed, random_state = classifier_random_state, tree_method = \"hist\", device = \"cpu\")\n",
    "\n",
    "if technology == \"sc\":\n",
    "\n",
    "    search_space_bayes = {\n",
    "        'learning_rate': (0.01, 0.3),                     # conservative to prevent noisy learning\n",
    "        'max_depth': (1, 14),                             # captures 4–6 feature interactions without overfitting\n",
    "        'min_child_weight': (1, 50),                      # helps avoid splits on small samples\n",
    "        'gamma': (0.1, 1.0, 'log-uniform'),               # require meaningful splits\n",
    "        'subsample': (0.6, 1.0),                          # stabilize with row sampling\n",
    "        'colsample_bytree': (0.5, 1.0),                   # feature subsampling per tree\n",
    "        'colsample_bylevel': (0.5, 1.0),                  # feature subsampling per split level\n",
    "        'reg_alpha': (1e-3, 10, 'log-uniform'),           # L1 penalty for sparsity\n",
    "        'reg_lambda': (0.1, 100, 'log-uniform'),          # L2 penalty to reduce overfitting\n",
    "        'max_delta_step': (0, 10),                        # improves convergence in imbalanced/unstable targets\n",
    "        'n_estimators': (100, 500),                       # enough boosting rounds for complexity\n",
    "        'objective': ['multi:softmax'],  \n",
    "        'booster': ['gbtree', 'dart'],  \n",
    "        'eval_metric': ['mlogloss', 'error', 'auc']\n",
    "    }\n",
    "\n",
    "else: \n",
    "\n",
    "    search_space_bayes = {\n",
    "        'learning_rate': (0.01, 0.3),                     # conservative to prevent noisy learning\n",
    "        'max_depth': (1, 14),                             # captures 4–6 feature interactions without overfitting\n",
    "        'min_child_weight': (1, 50),                      # helps avoid splits on small samples\n",
    "        'gamma': (0.1, 1.0, 'log-uniform'),               # require meaningful splits\n",
    "        'subsample': (0.6, 1.0),                          # stabilize with row sampling\n",
    "        'colsample_bytree': (0.5, 1.0),                   # feature subsampling per tree\n",
    "        'colsample_bylevel': (0.5, 1.0),                  # feature subsampling per split level\n",
    "        'reg_alpha': (1e-3, 10, 'log-uniform'),           # L1 penalty for sparsity\n",
    "        'reg_lambda': (0.1, 100, 'log-uniform'),          # L2 penalty to reduce overfitting\n",
    "        'max_delta_step': (0, 10),                        # improves convergence in imbalanced/unstable targets\n",
    "        'n_estimators': (100, 500),                       # enough boosting rounds for complexity\n",
    "        'objective': ['binary:logistic'],  \n",
    "        'booster': ['gbtree', 'dart'],  \n",
    "        'eval_metric': ['mlogloss', 'error', 'auc']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NESTED CV PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nested_CV_revenge_binary(\n",
    "    outer_cv, inner_cv,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model=classifier,\n",
    "    search_space=search_space_bayes,\n",
    "    n_jobs= n_jobs_sk, n_iter_bsearch = n_iter_bsearch,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "(\n",
    "    hyperparam_list,\n",
    "    selected_features_list,\n",
    "    accuracy_array,\n",
    "    f1_array,\n",
    "    precision_array,\n",
    "    recall_array,\n",
    "    roc_auc_array,\n",
    "    predictions_array,\n",
    "    probas_array\n",
    ") = results\n",
    "\n",
    "clear_output(wait = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The cross-validated accuracy (cv = {outer_cv}) is: {np.mean(accuracy_array)}\")\n",
    "print(f\"The cross-validated f1 (cv = {outer_cv}) is: {np.mean(f1_array)}\")\n",
    "print(f\"The cross-validated precision (cv = {outer_cv}) is: {np.mean(precision_array)}\")\n",
    "print(f\"The cross-validated recall (cv = {outer_cv}) is: {np.mean(recall_array)}\")\n",
    "print(f\"The cross-validated AUC (cv = {outer_cv}) is: {np.mean(roc_auc_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_in_at_least_one_cv = set([elem for inner_list in selected_features_list for elem in inner_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_grid = {}\n",
    "\n",
    "for dictionary in hyperparam_list:\n",
    "    for k,v in dictionary.items():\n",
    "        if k in search_space_grid:\n",
    "            search_space_grid[k].update([v])\n",
    "        else:\n",
    "            search_space_grid[k] = set([v])\n",
    "\n",
    "search_space_grid = {k : list(v) for k,v in search_space_grid.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = sk.model_selection.RandomizedSearchCV(estimator = classifier, \n",
    "                                             param_distributions = search_space_grid, \n",
    "                                             cv = inner_cv, \n",
    "                                             n_jobs = n_jobs_sk,\n",
    "                                             n_iter = n_iter_rsearch,\n",
    "                                             random_state = hypertune_random_state_rsearch, \n",
    "                                             verbose = 0, refit = True)\n",
    "opti.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = opti.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at both XGBoost native feature importance and at mean absolute SHAP values as metric of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_train)\n",
    "shap_values_numpy = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_shap:\n",
    "    print(\" / \".join([mode, condition]))\n",
    "    plt.figure(figsize = (100,25))\n",
    "    summary_plot = shap.summary_plot(shap_values, X_train, \n",
    "                                     plot_type='bar', max_display=10, plot_size= (25,10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform Feature Selection using mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model.feature_names_in_\n",
    "\n",
    "if technology == \"sc\":\n",
    "    feature_importances_shap = np.mean(np.mean(np.abs(explainer.shap_values(X_train)), axis = 0),axis = 1) # if sc I do multiclass\n",
    "else:\n",
    "    feature_importances_shap = np.mean(np.abs(explainer.shap_values(X_train)), axis = 0) # if sc I do multiclass\n",
    "\n",
    "X_train_reduced, X_test_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                            model, X_train, X_test, y_train, \n",
    "                                                                            inner_cv = inner_cv,\n",
    "                                                                            plot = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_discard = [feature for feature in selected_features if feature not in selected_features_in_at_least_one_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dropping columns never selected during Nested Cross Validation: {features_to_discard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train_reduced.drop(features_to_discard, axis = 1)\n",
    "X_test_reduced = X_test_reduced.drop(features_to_discard, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti_fs = sk.model_selection.RandomizedSearchCV(estimator = model, \n",
    "                                                param_distributions = search_space_grid, \n",
    "                                                cv = inner_cv, \n",
    "                                                n_jobs = n_jobs_sk,\n",
    "                                                n_iter = n_iter_rsearch,\n",
    "                                                random_state = hypertune_random_state_rsearch, \n",
    "                                                verbose = 0, refit = True)\n",
    "opti_fs.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fs = opti_fs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_fs = opti_fs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in best_params_fs.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = sk.metrics.make_scorer(sk.metrics.f1_score, average=\"macro\")\n",
    "precision_macro = sk.metrics.make_scorer(sk.metrics.precision_score, average=\"macro\")\n",
    "recall_macro = sk.metrics.make_scorer(sk.metrics.recall_score, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_array_fs = np.mean(sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = \"accuracy\"))\n",
    "f1_array_fs = np.mean(sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = f1_macro))\n",
    "precision_array_fs = np.mean(sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = precision_macro))\n",
    "recall_array_fs = np.mean(sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = recall_macro))\n",
    "roc_auc_array_fs = np.mean(sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = \"roc_auc_ovr\"))\n",
    "\n",
    "proba_predictions_fs = sk.model_selection.cross_val_predict(model_fs, X_train_reduced, y_train, cv = outer_cv, method= \"predict_proba\")\n",
    "class_array_fs = sk.model_selection.cross_val_predict(model_fs, X_train_reduced, y_train, cv = outer_cv, method= \"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = sk.preprocessing.LabelBinarizer().fit(y_train)\n",
    "y_onehot_train = label_binarizer.transform(y_train)\n",
    "n_classes = y_onehot_train.shape[1]\n",
    "target_names = np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = sk.metrics.roc_curve(y_onehot_train[:, i], proba_predictions_fs[:, i])\n",
    "    roc_auc[i] = sk.metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = fpr_grid\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = sk.metrics.auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score: {roc_auc['macro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    sk.metrics.RocCurveDisplay.from_predictions(\n",
    "        y_onehot_train[:, class_id],\n",
    "        proba_predictions_fs[:, class_id],\n",
    "        name=f\"ROC curve for {target_names[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "        plot_chance_level=(class_id == 2),\n",
    "        despine=True,\n",
    "    )\n",
    "\n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Variables for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_train_reduced,open(X_train_filename,\"wb\"))\n",
    "pickle.dump(X_test_reduced,open(X_test_filename,\"wb\"))\n",
    "pickle.dump(y_train,open(y_train_filename,\"wb\"))\n",
    "pickle.dump(y_test,open(y_test_filename,\"wb\"))\n",
    "pickle.dump(bin_train,open(bin_train_filename,\"wb\"))\n",
    "pickle.dump(bin_test,open(bin_test_filename,\"wb\"))\n",
    "pickle.dump(model_fs,open(model_filename,\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
