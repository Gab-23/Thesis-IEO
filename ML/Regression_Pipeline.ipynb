{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the python kernel we are using, should be the local one, not the one in the virtual environment. <br> \n",
    "NOTE: A proper virtual environment will be setup later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up main variables that will be called later on, for readibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_bool = True\n",
    "path_to_datasets = \"/home/ieo7429/Desktop/THESIS_GAB/outfiles/target_variables/ML_tables_regression_classification_BRCA_v2.RData\"\n",
    "mode = \"regrout\"\n",
    "cols_to_drop = [\"joint_probability\", \"bool_diff_acc\", \n",
    "                \"sign_mean_log2FC_1\", \"sign_mean_log2FC_2\", \"sign_mean_log2FC_3\", \n",
    "                \"weighted_coef_of_var_log2FC\", \"weighted_log2FC\", \"bin\", \"Type\"]\n",
    "\n",
    "if \"ampl_score\" in cols_to_drop and \"del_score\" in cols_to_drop:\n",
    "    condition = \"without_CNA\"\n",
    "else:\n",
    "    condition = \"with CNA\"\n",
    "\n",
    "n_jobs = 80\n",
    "nthread = 80\n",
    "n_iter_bsearch = 150\n",
    "n_iter_rsearch = 300\n",
    "\n",
    "split_random_state = 489574\n",
    "regressor_seed = 3737\n",
    "regressor_random_state = 39473209\n",
    "hypertune_random_state_bsearch = 3847\n",
    "hypertune_random_state_rsearch = 49574\n",
    "kfold_random_state = 4909\n",
    "\n",
    "outer_cv = 5\n",
    "inner_cv = 3\n",
    "plot_variation = False\n",
    "plot_gain = False\n",
    "plot_shap = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if install_bool:\n",
    "    ! pip install pickleshare\n",
    "    ! pip install pandas\n",
    "    ! pip install scikit-learn\n",
    "    ! pip install seaborn\n",
    "    ! pip install scipy\n",
    "    ! pip install --upgrade pip setuptools wheel\n",
    "    ! pip install rpy2\n",
    "    ! pip install shap\n",
    "    ! pip install \"numpy<=2.1\"\n",
    "    %pip install -U ipywidgets\n",
    "\n",
    "clear_output(wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import rpy2.robjects as ro\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings\n",
    "import random\n",
    "import pickleshare\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_datasets_from_r\n",
    "Reads \"ML.Tables\" .Rdata file and converts it to a nested dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_from_r(file_path):\n",
    "\n",
    "    \"\"\" \n",
    "    The function uses rpy2 to load datasets from an RData file.\n",
    "    In particular it expects a list of data.frames\n",
    "    It returns a dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    pandas2ri.activate()\n",
    "\n",
    "    ro.r['load'](file_path) # load the .RData\n",
    "\n",
    "    env = ro.globalenv # set the environment\n",
    "\n",
    "    r_list = env[list(env.keys())[0]] # take the list object\n",
    "\n",
    "    list_names = ro.r['names'](r_list) # take the names\n",
    "    list_names = [str(name) for name in list_names]\n",
    "\n",
    "    df_dict = {} # initialize outer dictionary\n",
    "    with (ro.default_converter + ro.pandas2ri.converter).context(): # start the conversion\n",
    "    # the conversion transforms a list of lists of lists of data.frames \n",
    "    # to a list of dictionaries of dictionaries of dictionaries of DataFrame\n",
    "        i = 0\n",
    "        for outer_dict in r_list:\n",
    "            df_dict[list_names[i]] = outer_dict\n",
    "            i += 1\n",
    "            \n",
    "    return(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_observed_predicted\n",
    "This function plots observed values and predicted values to give a quick visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_predicted(list_of_arrays, list_of_labels, title, x_label, y_label):\n",
    "\n",
    "    \"\"\"\n",
    "    The function takes a list of values and a list of labels as input and plots them.\n",
    "    NOTE: No genomic spatial relationship is preserved in the plot.\n",
    "    \"\"\"\n",
    "    n = len(list_of_arrays)\n",
    "\n",
    "    colors = sns.color_palette(\"dark\", n_colors = n)\n",
    "\n",
    "    for i in range(n):\n",
    "        color = colors[i]\n",
    "        sns.lineplot(list_of_arrays[i], color = color, label = list_of_labels[i])\n",
    "        sns.scatterplot(list_of_arrays[i], color = color)\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.xlabel(x_label, fontsize = 16)\n",
    "    plt.ylabel(y_label, fontsize = 16)\n",
    "    plt.legend(fontsize = 16)\n",
    "    plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows the computation of adjusted R^2, passing it as a custom scikit-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_r2_score(y_true, y_pred, **kwargs):\n",
    "    n, p = X.shape\n",
    "    r2 = sk.metrics.r2_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r2 = sk.metrics.make_scorer(adj_r2_score, response_method = \"predict\", greater_is_better= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_selector\n",
    "This function performs Incremental Feature Selection (IFS) to produce a reduced model <br> with best R2 performance with cross validation.\n",
    "Mean absolute SHAP values are used to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector_shap(feature_names, feature_importances, model, \n",
    "                          X_train, X_test, y_train, inner_cv,\n",
    "                          plot = True, verbose = True):\n",
    "\n",
    "    \"\"\"\n",
    "    The function takes as input feature names, feature importances, \n",
    "    a XGBoostRegressor object, the training and test dataset\n",
    "    and returns the reduced training and test datasets.\n",
    "    - plot parameter regulates whether or not to plot the R2 vs number of feature plot\n",
    "    - verbose parameter regulates whether or not to produce a text output\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    thresholds = np.sort(feature_importances) # sort the feature importances\n",
    "    num_features_list = [] # initialize list\n",
    "    r2_scores_list = [] # initialize list\n",
    "    \n",
    "    for threshold in thresholds: # iterate over thresholds\n",
    "        \n",
    "        vars_to_keep = np.where(feature_importances >= threshold)[0]\n",
    "        X_train_selected = X_train.iloc[:,vars_to_keep]\n",
    "\n",
    "        r2_scores = sk.model_selection.cross_val_score(estimator = model, \n",
    "                                                       X = X_train_selected, y = y_train, \n",
    "                                                       scoring = adjusted_r2, cv = inner_cv)\n",
    "        mean_r2_score = np.mean(r2_scores)\n",
    "        \n",
    "        num_features_list.append(X_train_selected.shape[1]) # append\n",
    "        r2_scores_list.append(mean_r2_score) # append\n",
    "\n",
    "        if verbose:\n",
    "            print(f'> threshold={threshold}, features={X_train_selected.shape[1]}, adj_R2={mean_r2_score}')\n",
    "        \n",
    "        if len(vars_to_keep) == 1:\n",
    "            break\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(num_features_list, r2_scores_list, marker='o')\n",
    "        plt.xlabel('Number of Selected Features')\n",
    "        plt.ylabel('cross validated mean R2')\n",
    "        plt.title('R2 vs. Number of Selected Features')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    optimal_threshold_index = np.argmax(r2_scores_list)\n",
    "    optimal_num_features = num_features_list[optimal_threshold_index]\n",
    "    \n",
    "    if (optimal_threshold_index) == 0:\n",
    "        optimal_threshold = 0\n",
    "    else:   \n",
    "        optimal_threshold = thresholds[optimal_threshold_index - 1]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"Number of Selected Features: {optimal_num_features}\")\n",
    "        print(f\"R2 at Optimal Threshold: {r2_scores_list[optimal_threshold_index]:.4f}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    n = 5\n",
    "    selected_features = feature_names[np.where(feature_importances > optimal_threshold)]\n",
    "    selected_features = [str(name) for name in selected_features]\n",
    "    discarded_features = [str(name) for name in feature_names if str(name) not in selected_features]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Selected features are: \\n\")\n",
    "        for i in range(0,len(selected_features), n):\n",
    "            print(\"  \".join(selected_features[i:i+n]))\n",
    "        print(\"\\n\")\n",
    "        print(\"Discarded features are: \\n\")\n",
    "        for i in range(0,len(discarded_features), n):\n",
    "            print(\"  \".join(discarded_features[i:i+n]))\n",
    "\n",
    "    X_train_reduced = X_train[selected_features]\n",
    "    X_test_reduced = X_test[selected_features]\n",
    "    \n",
    "    return(X_train_reduced, X_test_reduced, selected_features)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs nested cross validation, optimizing the model at each step in order to properly estimate the model's error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV(outer_cv, inner_cv,\n",
    "              X_train, y_train, \n",
    "              model, search_space, \n",
    "              n_jobs, n_iter_bsearch, verbose):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function performs nested cross validation to estimate model's error rate\n",
    "    It take the cv number, the training dataset, the target variable, the model or a BayesSearchCV\n",
    "    and a search space to perform hyperparameter tuning\n",
    "    \"\"\"\n",
    "\n",
    "    kf = sk.model_selection.KFold(n_splits = outer_cv, \n",
    "                                  shuffle = True, \n",
    "                                  random_state = kfold_random_state)\n",
    "    \n",
    "    mse_array = np.zeros(outer_cv)\n",
    "    r2_array = np.zeros(outer_cv)\n",
    "    predictions_array = np.zeros(len(X_train))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold #{i + 1}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 1 (Splitting)\")\n",
    "            \n",
    "        X_train_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 2 (Tuning)\")\n",
    "            \n",
    "        opti = BayesSearchCV(\n",
    "            estimator = model,\n",
    "            search_spaces = search_space,\n",
    "            n_iter = n_iter_bsearch,\n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True,\n",
    "            n_jobs = n_jobs,\n",
    "            verbose = False\n",
    "        )\n",
    "        \n",
    "        opti.fit(X_train_cv, y_train_cv)\n",
    "        best_model = opti.best_estimator_\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"STEP 3 (Prediction)\")\n",
    "            \n",
    "        cv_predictions = best_model.predict(X_val)\n",
    "        cv_mse = sk.metrics.mean_squared_error(y_val, cv_predictions)\n",
    "        cv_r2 = sk.metrics.r2_score(y_val, cv_predictions)\n",
    "        \n",
    "        mse_array[i] = cv_mse\n",
    "        r2_array[i] = cv_r2\n",
    "        predictions_array[val_index] = cv_predictions\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(mse_array, r2_array, predictions_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV la vendetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV_revenge(outer_cv, inner_cv, \n",
    "                      X_train, y_train, \n",
    "                      model, search_space, \n",
    "                      n_jobs, n_iter_bsearch, verbose):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function performs nested cross validation to estimate model's error rate\n",
    "    It take the cv number, the training dataset, the target variable, the model or a BayesSearchCV\n",
    "    and a search space to perform hyperparameter tuning\n",
    "    \"\"\"\n",
    "\n",
    "    kf = sk.model_selection.KFold(n_splits = outer_cv, \n",
    "                                  shuffle = True, \n",
    "                                  random_state = kfold_random_state)\n",
    "\n",
    "    hyperparam_list = []\n",
    "    selected_features_list = []\n",
    "    mse_array = np.zeros(outer_cv)\n",
    "    r2_array = np.zeros(outer_cv)\n",
    "    predictions_array = np.zeros(len(X_train))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold #{i + 1}\")\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"STEP 1 (Splitting)\")\n",
    "            \n",
    "        X_train_cv, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 2 (Tuning)\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.1 (HyperParameter Tuning Part 1)\")\n",
    "        \n",
    "        opti = BayesSearchCV(\n",
    "            estimator = model,\n",
    "            search_spaces = search_space,\n",
    "            n_iter = n_iter_bsearch,\n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True,\n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        opti.fit(X_train_cv, y_train_cv)\n",
    "        best_model = opti.best_estimator_\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.2 (SHAP)\")\n",
    "        \n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "        feature_names = best_model.feature_names_in_\n",
    "        feature_importances_shap = np.mean(np.abs(explainer.shap_values(X_train_cv)), axis = 0)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"STEP 2.3 (Feature selection)\")\n",
    "        \n",
    "        X_train_cv_reduced, X_val_cv_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                                        best_model, \n",
    "                                                                                        X_train_cv, X_val, y_train_cv, \n",
    "                                                                                        inner_cv,\n",
    "                                                                                        plot = False, verbose = False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"STEP 2.4 (HyperParameter Tuning Part 2)\")\n",
    "        \n",
    "        opti_fs = BayesSearchCV(\n",
    "            estimator = best_model, \n",
    "            search_spaces = search_space, \n",
    "            n_iter = n_iter_bsearch, \n",
    "            cv = inner_cv,\n",
    "            random_state = hypertune_random_state_bsearch,\n",
    "            refit = True, \n",
    "            n_jobs = n_jobs,\n",
    "            verbose = 0\n",
    "        )\n",
    "            \n",
    "        opti_fs.fit(X_train_cv_reduced, y_train_cv)\n",
    "        best_model_fs = opti_fs.best_estimator_\n",
    "        best_params_fs = opti_fs.best_params_\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"STEP 3 (Prediction)\")\n",
    "            \n",
    "        cv_predictions = best_model_fs.predict(X_val_cv_reduced)\n",
    "        cv_mse = sk.metrics.mean_squared_error(y_val, cv_predictions)\n",
    "        cv_r2 = sk.metrics.r2_score(y_val, cv_predictions)\n",
    "\n",
    "        hyperparam_list.append(best_params_fs)\n",
    "        selected_features_list.append(selected_features)\n",
    "        mse_array[i] = cv_mse\n",
    "        r2_array[i] = cv_r2\n",
    "        predictions_array[val_index] = cv_predictions\n",
    "       \n",
    "        i += 1\n",
    "\n",
    "    return(hyperparam_list, selected_features_list, \n",
    "           mse_array, r2_array, predictions_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the load_datasets_from_r function to load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = load_datasets_from_r(path_to_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = output_dict[mode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ids = working_df[[\"bin\"]]\n",
    "working_df = working_df.drop(cols_to_drop, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = working_df.iloc[:, 1:]\n",
    "y = working_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mean_log2FC\" in X.columns:\n",
    "    raise Exception(\"Target Variable in the training set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ampl_score</th>\n",
       "      <th>del_score</th>\n",
       "      <th>mean.GC.content</th>\n",
       "      <th>total_n_partners.trans</th>\n",
       "      <th>total_n_PPIs.trans_IntINSIDER</th>\n",
       "      <th>total_n_ohnologs.mmpaper_trans</th>\n",
       "      <th>total_n_paralogs_trans</th>\n",
       "      <th>dist.to.closest.FGS</th>\n",
       "      <th>Length_Counts.E1</th>\n",
       "      <th>Length_Counts.E10</th>\n",
       "      <th>...</th>\n",
       "      <th>distance.to.centromere</th>\n",
       "      <th>distance.to.telomere</th>\n",
       "      <th>Ess.distance_pancancer</th>\n",
       "      <th>ESSscore_pancancer</th>\n",
       "      <th>HAPLOscore_pancancer</th>\n",
       "      <th>Density.complex.proteins</th>\n",
       "      <th>Density.Ohnologs</th>\n",
       "      <th>Chromosome_Length</th>\n",
       "      <th>Centromere_Length</th>\n",
       "      <th>Centromere_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044986</td>\n",
       "      <td>0.320525</td>\n",
       "      <td>50.325833</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>61789940.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>22800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>114289940.0</td>\n",
       "      <td>6791450.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>249250621</td>\n",
       "      <td>2906265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075914</td>\n",
       "      <td>0.243674</td>\n",
       "      <td>36.060000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26299595.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24199400.0</td>\n",
       "      <td>96881990.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>249250621</td>\n",
       "      <td>2906265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074977</td>\n",
       "      <td>0.241799</td>\n",
       "      <td>38.824545</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27300601.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23198394.0</td>\n",
       "      <td>97882996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>249250621</td>\n",
       "      <td>2906265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076851</td>\n",
       "      <td>0.248360</td>\n",
       "      <td>39.422000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28301607.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22197388.0</td>\n",
       "      <td>98884002.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>249250621</td>\n",
       "      <td>2906265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.074039</td>\n",
       "      <td>0.234302</td>\n",
       "      <td>35.120000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29302613.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21196382.0</td>\n",
       "      <td>99885008.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>249250621</td>\n",
       "      <td>2906265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ampl_score  del_score  mean.GC.content  total_n_partners.trans  \\\n",
       "1    0.044986   0.320525        50.325833                     2.0   \n",
       "2    0.075914   0.243674        36.060000                     0.0   \n",
       "3    0.074977   0.241799        38.824545                     4.0   \n",
       "4    0.076851   0.248360        39.422000                    12.0   \n",
       "5    0.074039   0.234302        35.120000                     0.0   \n",
       "\n",
       "   total_n_PPIs.trans_IntINSIDER  total_n_ohnologs.mmpaper_trans  \\\n",
       "1                           52.0                            10.0   \n",
       "2                           12.0                             1.0   \n",
       "3                           54.0                             2.0   \n",
       "4                          650.0                             7.0   \n",
       "5                            6.0                             6.0   \n",
       "\n",
       "   total_n_paralogs_trans  dist.to.closest.FGS  Length_Counts.E1  \\\n",
       "1                    35.0           61789940.0            5400.0   \n",
       "2                     2.0           26299595.0            1000.0   \n",
       "3                     8.0           27300601.0            3600.0   \n",
       "4                    12.0           28301607.0            1900.0   \n",
       "5                     4.0           29302613.0               0.0   \n",
       "\n",
       "   Length_Counts.E10  ...  distance.to.centromere  distance.to.telomere  \\\n",
       "1            22800.0  ...             114289940.0             6791450.0   \n",
       "2             1400.0  ...              24199400.0            96881990.0   \n",
       "3             3500.0  ...              23198394.0            97882996.0   \n",
       "4                0.0  ...              22197388.0            98884002.0   \n",
       "5                0.0  ...              21196382.0            99885008.0   \n",
       "\n",
       "   Ess.distance_pancancer  ESSscore_pancancer  HAPLOscore_pancancer  \\\n",
       "1                     1.0            0.000000              0.416667   \n",
       "2                     1.0            0.000000              0.000000   \n",
       "3                     0.0            0.090909              0.090909   \n",
       "4                     1.0            0.000000              0.400000   \n",
       "5                     1.0            0.000000              0.000000   \n",
       "\n",
       "   Density.complex.proteins  Density.Ohnologs  Chromosome_Length  \\\n",
       "1                  0.083333          0.500000          249250621   \n",
       "2                  0.000000          1.000000          249250621   \n",
       "3                  0.090909          0.181818          249250621   \n",
       "4                  0.400000          0.200000          249250621   \n",
       "5                  0.000000          1.000000          249250621   \n",
       "\n",
       "   Centromere_Length  Centromere_Type  \n",
       "1            2906265                0  \n",
       "2            2906265                0  \n",
       "3            2906265                0  \n",
       "4            2906265                0  \n",
       "5            2906265                0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test: \n",
    "- In this case we do 70% training that will be processed by BayesSearchCV with cv = 5\n",
    "- The remaining 30% will be used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, bin_train, bin_test = sk.model_selection.train_test_split(X, y, bin_ids, \n",
    "                                                                                            test_size = 0.3, \n",
    "                                                                                            random_state = split_random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Regressors and the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to set the seed and random_state parameters in order to ensure reproducibility <br>\n",
    "NOTA: As it appears from the documentation, seed parameter is now deprecated, so set both seed and random_state to be sure\n",
    "\n",
    "Set up also the search_space for BayesSearchCV: They are large intervals, but BayesSearchCV is more efficient than regular GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(n_jobs = n_jobs, \n",
    "                             nthread = nthread, \n",
    "                             seed = regressor_seed, \n",
    "                             random_state = regressor_random_state)\n",
    "\n",
    "search_space_bayes = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': (0, 10),\n",
    "    'max_depth': (0, 50),\n",
    "    'max_delta_step': (0, 20),\n",
    "    'subsample': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "    'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "    'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "    'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "    'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "    'n_estimators': (50, 100),\n",
    "    'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NESTED CV PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "STEP 1 (Splitting)\n",
      "STEP 2 (Tuning)\n",
      "STEP 2.1 (HyperParameter Tuning Part 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2.2 (SHAP)\n",
      "STEP 2.3 (Feature selection)\n",
      "STEP 2.4 (HyperParameter Tuning Part 2)\n",
      "STEP 3 (Prediction)\n",
      "Fold #2\n",
      "STEP 1 (Splitting)\n",
      "STEP 2 (Tuning)\n",
      "STEP 2.1 (HyperParameter Tuning Part 1)\n",
      "STEP 2.2 (SHAP)\n",
      "STEP 2.3 (Feature selection)\n",
      "STEP 2.4 (HyperParameter Tuning Part 2)\n",
      "STEP 3 (Prediction)\n",
      "Fold #3\n",
      "STEP 1 (Splitting)\n",
      "STEP 2 (Tuning)\n",
      "STEP 2.1 (HyperParameter Tuning Part 1)\n"
     ]
    }
   ],
   "source": [
    "hyperparam_list, selected_features_list, mse_array, r2_array, predictions_array = nested_CV_revenge(outer_cv = outer_cv,\n",
    "                                                                                                    inner_cv = inner_cv,\n",
    "                                                                                                    X_train = X_train, \n",
    "                                                                                                    y_train = y_train, \n",
    "                                                                                                    model = regressor, \n",
    "                                                                                                    search_space = search_space_bayes,\n",
    "                                                                                                    n_jobs= n_jobs, n_iter_bsearch = n_iter_bsearch,\n",
    "                                                                                                    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The cross-validated root mean squared error (cv = {outer_cv}) is : {np.mean(np.sqrt(mse_array))}\")\n",
    "print(f\"The cross-validated R squared (R^2) (cv = {outer_cv}) is : {np.mean(r2_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_in_at_least_one_cv = set([elem for inner_list in selected_features_list for elem in inner_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_grid = {}\n",
    "\n",
    "for dictionary in hyperparam_list:\n",
    "    for k,v in dictionary.items():\n",
    "        if k in search_space_grid:\n",
    "            search_space_grid[k].update([v])\n",
    "        else:\n",
    "            search_space_grid[k] = set([v])\n",
    "\n",
    "search_space_grid = {k : list(v) for k,v in search_space_grid.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the true model error has beem estimated we can optimize the hyperparameter on the full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = sk.model_selection.RandomizedSearchCV(estimator = regressor, \n",
    "                                             param_distributions = search_space_grid, \n",
    "                                             cv = inner_cv, \n",
    "                                             n_jobs = n_jobs,\n",
    "                                             n_iter = n_iter_rsearch,\n",
    "                                             random_state = hypertune_random_state_rsearch, \n",
    "                                             verbose = 3, refit = True)\n",
    "opti.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = opti.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_variation:\n",
    "    plt.figure(figsize = (50,15))\n",
    "    plot_observed_predicted([y_train, predictions_array, model.predict(X_train)],\n",
    "                            [\"Observed\",\"CV Predicted\", \"Train Predicted\"], \n",
    "                            title = \"Observed VS CV Predicted VS Train Predicted\", \n",
    "                            x_label = \"Index\", y_label = \"mean_log2FC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at both XGBoost native feature importance and at mean absolute SHAP values as metric of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model, approximate= False)\n",
    "shap_values = explainer(X_train)\n",
    "shap_values_numpy = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_shap:\n",
    "    print(\" / \".join([mode, condition]))\n",
    "    plt.figure(figsize = (100,25))\n",
    "    summary_plot = shap.summary_plot(shap_values, X_train, \n",
    "                                     plot_type='bar', max_display=10, plot_size= (25,10), rng = 42)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform Feature Selection using mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model.feature_names_in_\n",
    "feature_importances_shap = np.mean(np.abs(explainer.shap_values(X_train)), axis = 0)\n",
    "\n",
    "X_train_reduced, X_test_reduced, selected_features = feature_selector_shap(feature_names, feature_importances_shap, \n",
    "                                                                            model, X_train, X_test, y_train, \n",
    "                                                                            inner_cv = inner_cv,\n",
    "                                                                            plot = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_discard = [feature for feature in selected_features if feature not in selected_features_in_at_least_one_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dropping columns never selected during Nested Cross Validation: {features_to_discard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train_reduced.drop(features_to_discard, axis = 1)\n",
    "X_test_reduced = X_test_reduced.drop(features_to_discard, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti_fs = sk.model_selection.RandomizedSearchCV(estimator = model, \n",
    "                                                param_distributions = search_space_grid, \n",
    "                                                cv = inner_cv, \n",
    "                                                n_jobs = n_jobs,\n",
    "                                                n_iter = n_iter_rsearch,\n",
    "                                                random_state = hypertune_random_state_rsearch, \n",
    "                                                verbose = 3, refit = True)\n",
    "opti_fs.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fs = opti_fs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2_cv = np.mean(sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = \"r2\"))\n",
    "mean_rmse_cv = np.mean(-sk.model_selection.cross_val_score(model_fs, X_train_reduced, y_train, cv = outer_cv, scoring = \"neg_root_mean_squared_error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cross Validated r2 is {mean_r2_cv}\")\n",
    "print(f\"Cross Validated rmse is {mean_rmse_cv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Variables for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_train_reduced\n",
    "%store X_test_reduced\n",
    "%store y_train\n",
    "%store y_test\n",
    "%store bin_train\n",
    "%store bin_test\n",
    "%store model_fs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
